import torch
from torch.distributions import Categorical
import torch.nn as nn
import torch.nn.functional as F
from utils.rl_utils import compute_gae

class PolicyNet(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_size=64):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, act_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)  # 输出 logits

class ValueNet(nn.Module):
    def __init__(self, obs_dim, hidden_size=64):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x).squeeze(-1)

class PPOAgent:
    def __init__(self, obs_dim, act_dim, clip_ratio=0.2, lr=3e-4, gamma=0.99, lam=0.95):
        self.policy_net = PolicyNet(obs_dim, act_dim)
        self.value_net  = ValueNet(obs_dim)
        self.clip_ratio = clip_ratio
        self.gamma      = gamma
        self.lam        = lam
        self.policy_optim = torch.optim.Adam(self.policy_net.parameters(), lr=lr)
        self.value_optim  = torch.optim.Adam(self.value_net.parameters(),  lr=lr)
    
    def select_action(self, obs):
        """
        输入 obs（numpy array 或 Tensor），
        返回 action(int), log_prob(float), value(float)
        """
        # 1. 转为 Tensor 并加 batch 维度
        if not isinstance(obs, torch.Tensor):
            obs = torch.tensor(obs, dtype=torch.float32)
        obs = obs.unsqueeze(0)  # shape [1, obs_dim]
        
        # 2. 策略网络输出 logits
        logits = self.policy_net(obs)  # [1, act_dim]
        
        # 3. 构造分布并采样
        dist = Categorical(logits=logits)
        action = dist.sample()  # [1]
        log_prob = dist.log_prob(action)  # [1]
        
        # 4. 价值网络估计
        value = self.value_net(obs)  # [1]
        
        # 5. 取标量返回
        return action.item(), log_prob.item(), value.item()
    
    def compute_gae(self, rewards, values, dones):
        """
        输入：
          rewards: list[float]，长度 T
          values:  list[float]，长度 T，V(s_t)
          dones:   list[bool or int]，长度 T，若该步结束则为 1，否则为 0
        返回：
          advantages, returns: 两个 list[float]，长度 T
        """
        T = len(rewards)
        advantages = [0.0] * T
        returns    = [0.0] * T
        gae = 0.0
        next_value = 0.0  # 末状态的值视为 0
        for t in reversed(range(T)):
            mask = 1.0 - float(dones[t])
            # δ_t = r_t + γ * V(s_{t+1}) * mask - V(s_t)
            delta = rewards[t] + self.gamma * next_value * mask - values[t]
            # GAE 递推：A_t = δ_t + γ * λ * mask * A_{t+1}
            gae = delta + self.gamma * self.lam * mask * gae
            advantages[t] = gae
            # Discounted return: R_t = A_t + V(s_t)
            returns[t] = gae + values[t]
            next_value = values[t]
        return advantages, returns


    def update(self, obs, acts, old_logps, advs, rets,
               clip_ratio=None, vf_coef=0.5, ent_coef=0.01):
        """
        一次 PPO-clip 更新
        obs, acts, old_logps, advs, rets: list 或 numpy array，长度 N
        clip_ratio: 可选覆盖 self.clip_ratio
        vf_coef:    value loss 权重
        ent_coef:   entropy bonus 权重
        """
        if clip_ratio is None:
            clip_ratio = self.clip_ratio

        # 转 Tensor
        obs_t   = torch.as_tensor(obs,      dtype=torch.float32)
        acts_t  = torch.as_tensor(acts,     dtype=torch.int64)
        oldlp_t = torch.as_tensor(old_logps, dtype=torch.float32)
        advs_t  = torch.as_tensor(advs,     dtype=torch.float32)
        rets_t  = torch.as_tensor(rets,     dtype=torch.float32)

        # 新策略分布与 log_prob
        logits   = self.policy_net(obs_t)
        dist     = Categorical(logits=logits)
        newlp_t  = dist.log_prob(acts_t)
        # 计算比例
        ratio    = torch.exp(newlp_t - oldlp_t)

        # 1) 策略损失：clip
        unclipped = ratio * advs_t
        clipped   = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * advs_t
        policy_loss = -torch.mean(torch.min(unclipped, clipped))

        # 2) 价值损失
        value_pred  = self.value_net(obs_t)
        value_loss  = torch.mean((value_pred - rets_t)**2)

        # 3) 熵奖励
        entropy = torch.mean(dist.entropy())

        # 总 loss
        loss = policy_loss + vf_coef * value_loss - ent_coef * entropy

        # 反向 + 更新
        self.policy_optim.zero_grad()
        self.value_optim.zero_grad()
        loss.backward()
        self.policy_optim.step()
        self.value_optim.step()

        return {
            "policy_loss": policy_loss.item(),
            "value_loss":  value_loss.item(),
            "entropy":     entropy.item(),
            "ratio_mean":  ratio.mean().item(),
        }
